# Dashboard Seccionadora - LCDC Mendoza

## ğŸ“‹ DescripciÃ³n General

Sistema integral de anÃ¡lisis y monitoreo para mÃ¡quina seccionadora de MDF que permite visualizar mÃ©tricas de producciÃ³n, eficiencia operativa y tiempo ocioso en tiempo real. La aplicaciÃ³n procesa logs de mÃ¡quina y presenta dashboards interactivos para toma de decisiones basada en datos.

## ğŸ¯ Funcionalidades Principales

### ğŸ“ˆ Overview General de ProducciÃ³n
- **KPIs Principales**: Total esquemas ejecutados, placas procesadas, placas blancas 18mm, promedio placas/dÃ­a
- **KPIs de Eficiencia**: Promedio minutos/esquema, horas mÃ¡quina, placas por hora, porcentaje tiempo ocioso
- **Tendencias Diarias**: GrÃ¡ficos temporales de esquemas, placas, horas mÃ¡quina y eficiencia
- **AnÃ¡lisis Temporal**: VisualizaciÃ³n de tiempo ocioso y su impacto en la productividad
- **DistribuciÃ³n por Espesores**: AnÃ¡lisis comparativo de materiales procesados

### ğŸ“… AnÃ¡lisis Diario de ProducciÃ³n
- **Filtros Temporales**: SelecciÃ³n de rangos de fechas personalizados
- **MÃ©tricas del PerÃ­odo**: ResÃºmenes estadÃ­sticos del perÃ­odo seleccionado
- **EvoluciÃ³n Temporal**: 6 grÃ¡ficos simultÃ¡neos (esquemas, placas, horas mÃ¡quina, eficiencia, tiempo ocioso, placas 18mm)
- **AnÃ¡lisis de Correlaciones**: Relaciones entre horas mÃ¡quina vs eficiencia y tiempo ocioso vs productividad
- **Tabla Detallada**: Datos granulares por dÃ­a con mÃ©tricas calculadas

### âš¡ AnÃ¡lisis por Espesores
- **Resumen por Material**: EstadÃ­sticas de espesores mÃ¡s utilizados y eficientes
- **ComparaciÃ³n Visual**: GrÃ¡ficos de distribuciÃ³n y tiempos promedio
- **AnÃ¡lisis de Eficiencia**: Scatter plot relacionando duraciÃ³n vs volumen de producciÃ³n
- **Insights AutomÃ¡ticos**: IdentificaciÃ³n de materiales mÃ¡s y menos eficientes
- **Tabla Detallada**: MÃ©tricas por espesor incluyendo placas/esquema y Ã¡rea procesada

### ğŸ”§ AnÃ¡lisis por Jobs
- **Filtros DinÃ¡micos**: Top N jobs ordenables por diferentes criterios
- **Visualizaciones Comparativas**: GrÃ¡ficos de barras horizontales para top jobs
- **AnÃ¡lisis de DistribuciÃ³n**: Scatter plots y histogramas de rendimiento
- **AnÃ¡lisis Dimensional**: RelaciÃ³n Ã¡rea de corte vs duraciÃ³n y distribuciÃ³n por espesor
- **Insights de Rendimiento**: IdentificaciÃ³n automÃ¡tica de jobs mÃ¡s eficientes y productivos

## ğŸ—ï¸ Arquitectura TÃ©cnica

### Stack TecnolÃ³gico
```
Frontend: Streamlit (Python)
Backend: PostgreSQL
VisualizaciÃ³n: Plotly Express/Graph Objects
ETL: Pandas + SQLAlchemy
Deployment: Streamlit Community Cloud
```

### Arquitectura de Datos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Log Files     â”‚    â”‚     ETL      â”‚    â”‚   PostgreSQL    â”‚
â”‚   (.txt)        â”‚â”€â”€â”€â–¶â”‚  Processing  â”‚â”€â”€â”€â–¶â”‚   Database      â”‚
â”‚                 â”‚    â”‚              â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                       â”‚
                              â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Processed     â”‚    â”‚  Data Cache  â”‚    â”‚   Streamlit     â”‚
â”‚   Archive       â”‚    â”‚  (5 min TTL) â”‚    â”‚   Dashboard     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Base de Datos

#### Tabla Principal: `cortes_seccionadora`
```sql
- id: SERIAL PRIMARY KEY
- nombre_optimizacion: VARCHAR(255) -- Nombre del archivo de esquema
- job_key: VARCHAR(255) -- Identificador Ãºnico del trabajo
- fecha_proceso: DATE -- Fecha de procesamiento
- hora_inicio/hora_fin: TIME -- Timestamps de ejecuciÃ³n
- largo_mm, ancho_mm, espesor_mm: DECIMAL -- Dimensiones
- cantidad_placas: INTEGER -- Placas procesadas en el esquema
- duracion_segundos: INTEGER (CALCULATED) -- DuraciÃ³n automÃ¡tica
- area_mm2, volumen_mm3: DECIMAL (CALCULATED) -- MÃ©tricas calculadas
```

#### Vistas AnalÃ­ticas Preconstruidas
- **`metricas_diarias`**: Agregaciones por fecha con KPIs diarios
- **`analisis_por_job`**: MÃ©tricas agrupadas por trabajo Ãºnico
- **`eficiencia_por_espesor`**: EstadÃ­sticas por tipo de material

### Componentes del Sistema

#### 1. ETL Pipeline (`etl_secc.py`)
```python
def main():
    # 1. Procesar archivos de log nuevos
    # 2. Transformar datos para PostgreSQL
    # 3. Cargar a base de datos
    # 4. Archivar archivos procesados
```

#### 2. Parser de Logs (`script_seccionadora.py`)
```python
def procesar_archivo(filepath):
    # Parsea lÃ­neas del formato:
    # LOG=ruta,largo,ancho,espesor,min,hora,seg,mes,aÃ±o,dÃ­a,hora_fin,min_fin,seg_fin,placas
    # Extrae job_key preciso eliminando extensiÃ³n
    # Convierte a DataFrame estructurado
```

#### 3. Dashboard Principal (`dashboard_streamlit.py`)
- **ConexiÃ³n**: SQLAlchemy con cache de 5 minutos
- **Consultas**: SQL optimizado con CTEs y window functions
- **VisualizaciÃ³n**: Plotly con mÃºltiples tipos de grÃ¡ficos
- **Interactividad**: Filtros, selecciÃ³n de perÃ­odos, navegaciÃ³n

## ğŸ”„ Flujo de Trabajo

### 1. Ingesta de Datos
```
Logs de MÃ¡quina (.txt) â†’ Carpeta archivos_entrada/
```

### 2. Procesamiento ETL
```bash
python etl_secc.py
```
- Lee archivos de `archivos_entrada/`
- Parsea logs con `script_seccionadora.py`
- Transforma datos (fecha, hora, dimensiones)
- Inserta en PostgreSQL
- Mueve archivos a `procesados/` con timestamp

### 3. VisualizaciÃ³n
```bash
streamlit run dashboard_streamlit.py
```
- Conecta a PostgreSQL
- Cache de consultas (5 min TTL)
- Renderiza dashboards interactivos
- ActualizaciÃ³n automÃ¡tica de datos

### 4. Deployment
```
GitHub â†’ Streamlit Community Cloud â†’ ProducciÃ³n
```

## ğŸ“Š MÃ©tricas y KPIs Clave

### KPIs Operativos
- **Total Esquemas**: NÃºmero de trabajos ejecutados
- **Placas Procesadas**: Volumen total de producciÃ³n
- **Placas Blancas 18mm**: MÃ©trica especÃ­fica de material principal
- **Promedio Placas/DÃ­a**: Capacidad diaria promedio

### KPIs de Eficiencia
- **Promedio Min/Esquema**: Tiempo promedio por trabajo
- **Horas MÃ¡quina**: Tiempo productivo real
- **Placas por Hora**: Throughput de producciÃ³n
- **Tiempo Ocioso %**: Eficiencia temporal de la mÃ¡quina

### CÃ¡lculo de Tiempo Ocioso
```sql
tiempo_span = MAX(hora_fin) - MIN(hora_inicio)  -- Tiempo total del dÃ­a
tiempo_productivo = SUM(duracion_segundos)       -- Tiempo trabajando
tiempo_ocioso = tiempo_span - tiempo_productivo  -- Tiempo muerto
porcentaje_ocioso = (tiempo_ocioso / tiempo_span) * 100
```

## ğŸ› ï¸ ConfiguraciÃ³n y Deploy

### Requisitos del Sistema
```
Python 3.8+
PostgreSQL 12+
Dependencias: requirements.txt
```

### Variables de Entorno
```env
PG_CONN=postgresql://user:password@host:port/seccionadora_logs
```

### Deployment en Streamlit Cloud
1. **PreparaciÃ³n**: Subir cÃ³digo a GitHub (pÃºblico)
2. **ConexiÃ³n**: Vincular repositorio en share.streamlit.io
3. **ConfiguraciÃ³n**: Agregar `PG_CONN` en secrets
4. **Deploy**: AutomÃ¡tico en cada push a main

### Estructura de Archivos
```
Seccionadora/
â”œâ”€â”€ dashboard_streamlit.py      # Dashboard principal
â”œâ”€â”€ etl_secc.py                # Pipeline ETL
â”œâ”€â”€ script_seccionadora.py     # Parser de logs
â”œâ”€â”€ init_database.sql          # Schema de BD
â”œâ”€â”€ requirements.txt           # Dependencias
â”œâ”€â”€ .streamlit/config.toml     # ConfiguraciÃ³n UI
â”œâ”€â”€ README.md                  # Esta documentaciÃ³n
â”œâ”€â”€ README_DEPLOYMENT.md       # GuÃ­a de deployment
â”œâ”€â”€ .env.example              # Plantilla de variables
â”œâ”€â”€ archivos_entrada/         # Logs nuevos
â””â”€â”€ procesados/              # Logs archivados
```

## ğŸ”’ Consideraciones de Seguridad

- **Variables de Entorno**: Credenciales no hardcodeadas
- **SSL**: Conexiones encriptadas automÃ¡ticas
- **Cache**: TTL de 5 minutos para datos sensibles
- **Logs**: Sin exposiciÃ³n de informaciÃ³n confidencial

## ğŸš€ Mejoras Futuras

### Funcionalidades
- [ ] Alertas automÃ¡ticas por baja eficiencia
- [ ] PredicciÃ³n de mantenimiento
- [ ] Reportes PDF automatizados
- [ ] API REST para integraciÃ³n

### TÃ©cnico
- [ ] MigraciÃ³n a base de datos en la nube
- [ ] ImplementaciÃ³n de CI/CD
- [ ] Tests automatizados
- [ ] Monitoreo de aplicaciÃ³n

## ğŸ“ Soporte

Para problemas tÃ©cnicos:
1. Verificar logs en Streamlit Cloud dashboard
2. Validar conexiÃ³n a PostgreSQL
3. Revisar formato de archivos de log
4. Contactar al desarrollador para soporte especializado

---

**Desarrollado para LCDC Mendoza - Sistema de AnÃ¡lisis de Seccionadora MDF**